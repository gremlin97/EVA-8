{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gremlin97/EVA-8/blob/main/S5/Eva3_Step2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target:**\n",
        "\n",
        "Added LR Schdeuler with gamma=0.1 and step=6, reduced channel size by reducing the number of kernels for each convolution block. Pushed parms below 10k. Increased learning rate to increasing learning for epochs below 15 and to offset the regularization. Removed Padding=1 to reduce feature map size faster. Added random rotation to image of 7 degrees\n",
        "\n",
        "**Results**:\n",
        "\n",
        "* Parameters: 9,866\n",
        "* Best Train Accuracy: 98.68\n",
        "* Best Test Accuracy: 99.23\n",
        "\n",
        "**Analysis:**\n",
        "I was able to reduce the model parameters below 10k by reducing the number of filter and maintaining the number of out channels as 16 after each channel. The train accuracy was lower by the test accuracy by around 1% indicating that my model can learn more and achieve higher accuracy. The learning has become harder to to the multiple form of regularizations (dropout, random rotations). Somehow I need to increase the learning."
      ],
      "metadata": {
        "id": "p9JbpMVpWnV4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=0) # RF:1+(3-1)1=3; ji=1,jo=1; 28x28x1 -> 26x26x16\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # RF:3+(2-1)1=4; ji=1,jo=2; 26x26x16 -> 13x13x16\n",
        "        self.drop1 = nn.Dropout(0.1)\n",
        "        self.conv3 = nn.Conv2d(16, 16, 3, padding=0) # RF:4+(3-1)2=8; ji=2,jo=2; 13x13x16 -> 11x11x16\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # RF:8+(2-1)2=10; ji=2,jo=4; 11x11x16 -> 5x5x16\n",
        "        self.drop2 = nn.Dropout(0.1)\n",
        "        self.conv5 = nn.Conv2d(16, 16, 3, padding=1) # RF:10+(3-1)4=18; ji=4,jo=4; 5x5x16 -> 5x5x16\n",
        "        self.bn3 = nn.BatchNorm2d(16) \n",
        "        self.pool3 = nn.MaxPool2d(2, 2) # RF:18+(2-1)4=22; ji=4,jo=8; 5x5x16 -> 2x2x16\n",
        "        self.drop3 = nn.Dropout(0.1)\n",
        "        self.conv6 = nn.Conv2d(16, 32, 3, padding=1) # RF:22+(3-1)8=38; ji=4,jo=8; 2x2x16 -> 2x2x32\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1,1)) \n",
        "\n",
        "        self.lin = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv6(self.drop3(self.pool3(self.bn3(F.relu(self.conv5(self.drop2(self.pool2(self.bn2(F.relu(self.conv3(F.relu(self.drop1(self.pool1(self.bn1(F.relu(self.conv1(x)))))))))))))))))\n",
        "\n",
        "        x = self.gap(x)\n",
        "\n",
        "        x = x.view(-1, 32)\n",
        "\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        # x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "DonoZqykydfJ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "out = model(torch.randn(1,1,28,28))\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUBgTH4IQZmT",
        "outputId": "c6cd380e-c4b1-4f6a-c248-db9e3145dbc9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-7f12a01f2234>:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2cSqWH6Ywmy",
        "outputId": "06513556-5392-4800-af01-4b92f749709d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (drop1): Dropout(p=0.1, inplace=False)\n",
              "  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (drop2): Dropout(p=0.1, inplace=False)\n",
              "  (conv5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (drop3): Dropout(p=0.1, inplace=False)\n",
              "  (conv6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (lin): Linear(in_features=32, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3095661-8df5-49b5-bd51-5a6f53e90cc7"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             160\n",
            "       BatchNorm2d-2           [-1, 16, 26, 26]              32\n",
            "         MaxPool2d-3           [-1, 16, 13, 13]               0\n",
            "           Dropout-4           [-1, 16, 13, 13]               0\n",
            "            Conv2d-5           [-1, 16, 11, 11]           2,320\n",
            "       BatchNorm2d-6           [-1, 16, 11, 11]              32\n",
            "         MaxPool2d-7             [-1, 16, 5, 5]               0\n",
            "           Dropout-8             [-1, 16, 5, 5]               0\n",
            "            Conv2d-9             [-1, 16, 5, 5]           2,320\n",
            "      BatchNorm2d-10             [-1, 16, 5, 5]              32\n",
            "        MaxPool2d-11             [-1, 16, 2, 2]               0\n",
            "          Dropout-12             [-1, 16, 2, 2]               0\n",
            "           Conv2d-13             [-1, 32, 2, 2]           4,640\n",
            "AdaptiveAvgPool2d-14             [-1, 32, 1, 1]               0\n",
            "           Linear-15                   [-1, 10]             330\n",
            "================================================================\n",
            "Total params: 9,866\n",
            "Trainable params: 9,866\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.25\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.29\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-7f12a01f2234>:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.RandomRotation((-5.0, 5.0), fill=(1,)),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx} Train Accuracy={100. * correct / len(train_loader.dataset)}')\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Test Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2114e598-8b6d-497a-c9cd-429e9c11b97c"
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9)\n",
        "scheduler =  optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "#lr=0.01\n",
        "\n",
        "for epoch in range(15):\n",
        "    print(\"Epoch: \",epoch+1)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1875 [00:00<?, ?it/s]<ipython-input-44-7f12a01f2234>:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.17063243687152863 batch_id=1874 Train Accuracy=92.93166666666667: 100%|██████████| 1875/1875 [00:30<00:00, 60.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0894, Test Accuracy: 9712/10000 (97%)\n",
            "\n",
            "Epoch:  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.03233985975384712 batch_id=1874 Train Accuracy=96.065: 100%|██████████| 1875/1875 [00:30<00:00, 61.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0744, Test Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Epoch:  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.5031666159629822 batch_id=1874 Train Accuracy=96.65: 100%|██████████| 1875/1875 [00:30<00:00, 60.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0731, Test Accuracy: 9773/10000 (98%)\n",
            "\n",
            "Epoch:  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.1628304421901703 batch_id=1874 Train Accuracy=96.945: 100%|██████████| 1875/1875 [00:30<00:00, 61.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0544, Test Accuracy: 9836/10000 (98%)\n",
            "\n",
            "Epoch:  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.06440648436546326 batch_id=1874 Train Accuracy=97.0: 100%|██████████| 1875/1875 [00:30<00:00, 60.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0684, Test Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Epoch:  6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.14367364346981049 batch_id=1874 Train Accuracy=97.16166666666666: 100%|██████████| 1875/1875 [00:30<00:00, 61.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0623, Test Accuracy: 9825/10000 (98%)\n",
            "\n",
            "Epoch:  7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.04679401218891144 batch_id=1874 Train Accuracy=98.38333333333334: 100%|██████████| 1875/1875 [00:30<00:00, 61.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0343, Test Accuracy: 9885/10000 (99%)\n",
            "\n",
            "Epoch:  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.02947418764233589 batch_id=1874 Train Accuracy=98.525: 100%|██████████| 1875/1875 [00:30<00:00, 60.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0367, Test Accuracy: 9890/10000 (99%)\n",
            "\n",
            "Epoch:  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.07044631987810135 batch_id=1874 Train Accuracy=98.56: 100%|██████████| 1875/1875 [00:30<00:00, 60.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0338, Test Accuracy: 9895/10000 (99%)\n",
            "\n",
            "Epoch:  10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.13199523091316223 batch_id=1874 Train Accuracy=98.50666666666666: 100%|██████████| 1875/1875 [00:31<00:00, 60.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0333, Test Accuracy: 9892/10000 (99%)\n",
            "\n",
            "Epoch:  11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.006351368501782417 batch_id=1874 Train Accuracy=98.50333333333333: 100%|██████████| 1875/1875 [00:30<00:00, 61.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0320, Test Accuracy: 9907/10000 (99%)\n",
            "\n",
            "Epoch:  12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0238038320094347 batch_id=1874 Train Accuracy=98.58666666666667: 100%|██████████| 1875/1875 [00:30<00:00, 60.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0305, Test Accuracy: 9895/10000 (99%)\n",
            "\n",
            "Epoch:  13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.15996260941028595 batch_id=1874 Train Accuracy=98.61166666666666: 100%|██████████| 1875/1875 [00:32<00:00, 57.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0313, Test Accuracy: 9892/10000 (99%)\n",
            "\n",
            "Epoch:  14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.11246927827596664 batch_id=1874 Train Accuracy=98.67333333333333: 100%|██████████| 1875/1875 [00:32<00:00, 58.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0312, Test Accuracy: 9895/10000 (99%)\n",
            "\n",
            "Epoch:  15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.004021728876978159 batch_id=1874 Train Accuracy=98.63333333333334: 100%|██████████| 1875/1875 [00:31<00:00, 60.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0303, Test Accuracy: 9903/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Training_Logs = '''\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "FUbt_JGQFDfP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H6gH0nYxffHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6SkjOzofv7k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}