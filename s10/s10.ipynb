{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBzx8EtKFBg+1yfzQb+goM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gremlin97/EVA-8/blob/main/s10/s10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prc7i8nWvndU",
        "outputId": "b724722c-85a2-48cc-a465-0f36cc794e08"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bgLwt-0Xu55e"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F  \n",
        "from torchinfo import summary   \n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes on Output Dimensions:**\n",
        "\n",
        "```\n",
        "hxwx3 => 224x224x3 => P=16\n",
        "\n",
        "14x14x3 => 14x14x512\n",
        "\n",
        "h=8\n",
        "\n",
        "14x14x8x64\n",
        "\n",
        "bx196xdim=512\n",
        "\n",
        "1x196x512\n",
        "\n",
        "1x14x14x512\n",
        "```"
      ],
      "metadata": {
        "id": "RP6hwTHfeONi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,num_patches,shape,in_channels=3,patch_size=16):\n",
        "    super().__init__()\n",
        "    self.patch = nn.Conv2d(in_channels,out_channels=embedding_dim,kernel_size=patch_size,stride=patch_size,padding=0) # b x dim x ph x pw == bx512x14x14\n",
        "    #self.flatten = nn.Flatten(start_dim = 2, end_dim=-1) # bx512x196\n",
        "    # self.cls_token = nn.Parameter(torch.randn(1,1,embedding_dim),requires_grad=True) # bx197x512 (Permuted: 0,2,1)\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(1,embedding_dim,shape,shape),requires_grad=True) # b x 197 x 512\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,_,h,w = x.shape\n",
        "    x = self.patch(x)\n",
        "    #x = self.flatten(x)\n",
        "    # x = x.permute(0,2,1)\n",
        "    # b_cls_token = self.cls_token.expand(b,-1,-1)\n",
        "    # x = torch.cat((b_cls_token,x),dim=1)\n",
        "    x = x + self.pos_embedding\n",
        "    # print(x.shape)\n",
        "    # x = x.reshape([b,-1,h,w])\n",
        "    return x"
      ],
      "metadata": {
        "id": "5eDZaK3JfjWA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MhSelfAttention(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels=512, head_channels=8):\n",
        "    super().__init__()\n",
        "    self.head_channels = head_channels\n",
        "    self.heads = out_channels//head_channels\n",
        "    self.scale = head_channels**-0.5\n",
        "\n",
        "    self.k = nn.Conv2d(in_channels,out_channels,1)\n",
        "    self.q = nn.Conv2d(in_channels,out_channels,1)\n",
        "    self.v = nn.Conv2d(in_channels,out_channels,1)\n",
        "\n",
        "    self.unify_heads = nn.Conv2d(out_channels,out_channels,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    b,_,h,w = x.shape\n",
        "    keys = self.k(x).view(b,self.heads,self.head_channels,-1) # bx8x64x196\n",
        "    queries = self.q(x).view(b,self.heads,self.head_channels,-1)\n",
        "    values = self.v(x).view(b,self.heads,self.head_channels,-1) \n",
        "\n",
        "    # print(\"K\",keys.shape)\n",
        "    # print(\"Q\",queries.shape)\n",
        "    # print(\"V\",values.shape)\n",
        "\n",
        "    attend = queries @ keys.transpose(-2,-1)*self.scale\n",
        "    attention = F.softmax(attend,dim=-2) # Softmax along the channels (0-1)\n",
        "\n",
        "    out = attention @ values\n",
        "    out = out.view(b,-1,h,w)\n",
        "    out = self.unify_heads(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "JCRadkjkviTQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,multf=4):\n",
        "    super().__init__()\n",
        "    hidden_channels = in_channels * multf\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,hidden_channels,1),\n",
        "        nn.GELU(),\n",
        "        nn.Conv2d(hidden_channels,out_channels,1)\n",
        "    )\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x = self.net(x)\n",
        "    return x      "
      ],
      "metadata": {
        "id": "kNFnxNTmRZox"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNormChannels(nn.Module):\n",
        "  def __init__(self,embedding_dim,fn):\n",
        "    super().__init__()\n",
        "    self.lnorm = nn.LayerNorm(embedding_dim)\n",
        "    self.fn = fn\n",
        "  \n",
        "  def forward(self,x,**kwargs):\n",
        "    x = x.transpose(1, -1)\n",
        "    x = self.lnorm(x)\n",
        "    x = self.fn(x.transpose(-1, 1),**kwargs)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "RnOZrJGpYmrN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,depth,head_c,in_c,out_c):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.ModuleList([])\n",
        "    for _ in range(depth):\n",
        "      self.layers.append(nn.ModuleList([\n",
        "          PreNormChannels(out_c,MhSelfAttention(in_channels=in_c, out_channels=out_c,head_channels=head_c)),\n",
        "          PreNormChannels(out_c,FeedForward(in_c,out_c,4))\n",
        "      ]))\n",
        "\n",
        "  def forward(self,x):\n",
        "    for attn,ff in self.layers:\n",
        "      x = attn(x) + x\n",
        "      x = ff(x) + x\n",
        "    \n",
        "    return x"
      ],
      "metadata": {
        "id": "dEqUO-3CUvtl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self,image_size,patch_size,in_channels,embedding_dim,depth,head_channels):\n",
        "    super().__init__()\n",
        "    reduced_size = image_size//patch_size\n",
        "\n",
        "    image_height = image_size\n",
        "    image_width = image_size\n",
        "\n",
        "    num_patches = (image_height*image_width)//(patch_size*patch_size)\n",
        "\n",
        "    self.embedding = Embedding(embedding_dim,num_patches,reduced_size,in_channels,patch_size)\n",
        "    self.transformer = Transformer(depth,head_channels,embedding_dim,embedding_dim)\n",
        "    self.norm =  nn.LayerNorm(embedding_dim)\n",
        "    self.mlp_head = nn.Sequential(\n",
        "          nn.GELU(),\n",
        "          nn.AdaptiveAvgPool2d(1),\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(embedding_dim, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.transformer(x)\n",
        "    x = x.transpose(1,-1)\n",
        "    x = self.norm(x)\n",
        "    x = x.transpose(-1,1)\n",
        "    # print(x.shape)\n",
        "    x = self.mlp_head(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "17kY_fJjrzgB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "2AC_43TE0qPM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz1aSR-SN7EU",
        "outputId": "e93ef139-e274-472a-f579-9af2e257187f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT(32,2,3,32,4,8)\n",
        "# image_size,patch_size,in_channels,embedding_dim,depth,head_channels\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_HUgwD2zFpw",
        "outputId": "5f72b193-0d3f-45c6-a252-93905759d476"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (embedding): Embedding(\n",
              "    (patch): Conv2d(3, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layers): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): PreNormChannels(\n",
              "          (lnorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): MhSelfAttention(\n",
              "            (k): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (q): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (v): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unify_heads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): PreNormChannels(\n",
              "          (lnorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0): PreNormChannels(\n",
              "          (lnorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): MhSelfAttention(\n",
              "            (k): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (q): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (v): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unify_heads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): PreNormChannels(\n",
              "          (lnorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0): PreNormChannels(\n",
              "          (lnorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): MhSelfAttention(\n",
              "            (k): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (q): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (v): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unify_heads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): PreNormChannels(\n",
              "          (lnorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): ModuleList(\n",
              "        (0): PreNormChannels(\n",
              "          (lnorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): MhSelfAttention(\n",
              "            (k): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (q): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (v): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unify_heads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): PreNormChannels(\n",
              "          (lnorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "  (mlp_head): Sequential(\n",
              "    (0): GELU(approximate='none')\n",
              "    (1): AdaptiveAvgPool2d(output_size=1)\n",
              "    (2): Flatten(start_dim=1, end_dim=-1)\n",
              "    (3): Linear(in_features=32, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(torch.randn(1,3,32,32).to(device))\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GngGqJV12bac",
        "outputId": "2f78434f-48a5-4c51-f30a-b14bc50d1ccc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2405,  0.0830,  0.1412,  0.1047, -0.4084, -0.0204, -0.3082, -0.3534,\n",
            "         -0.1373,  0.2211]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_input_image = (1, 3, 32, 32)\n",
        "summary(model,\n",
        "        input_size=random_input_image, # try swapping this for \"random_input_image_error\" \n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-HVzsRs0sew",
        "outputId": "c5d3bf31-2364-42e1-94ce-058caa1578dc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=======================================================================================================================================\n",
              "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
              "=======================================================================================================================================\n",
              "ViT (ViT)                                               [1, 3, 32, 32]       [1, 10]              --                   True\n",
              "├─Embedding (embedding)                                 [1, 3, 32, 32]       [1, 32, 16, 16]      8,192                True\n",
              "│    └─Conv2d (patch)                                   [1, 3, 32, 32]       [1, 32, 16, 16]      416                  True\n",
              "├─Transformer (transformer)                             [1, 32, 16, 16]      [1, 32, 16, 16]      --                   True\n",
              "│    └─ModuleList (layers)                              --                   --                   --                   True\n",
              "│    │    └─ModuleList (0)                              --                   --                   12,704               True\n",
              "│    │    └─ModuleList (1)                              --                   --                   12,704               True\n",
              "│    │    └─ModuleList (2)                              --                   --                   12,704               True\n",
              "│    │    └─ModuleList (3)                              --                   --                   12,704               True\n",
              "├─LayerNorm (norm)                                      [1, 16, 16, 32]      [1, 16, 16, 32]      64                   True\n",
              "├─Sequential (mlp_head)                                 [1, 32, 16, 16]      [1, 10]              --                   True\n",
              "│    └─GELU (0)                                         [1, 32, 16, 16]      [1, 32, 16, 16]      --                   --\n",
              "│    └─AdaptiveAvgPool2d (1)                            [1, 32, 16, 16]      [1, 32, 1, 1]        --                   --\n",
              "│    └─Flatten (2)                                      [1, 32, 1, 1]        [1, 32]              --                   --\n",
              "│    └─Linear (3)                                       [1, 32]              [1, 10]              330                  True\n",
              "=======================================================================================================================================\n",
              "Total params: 59,818\n",
              "Trainable params: 59,818\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 12.99\n",
              "=======================================================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 3.01\n",
              "Params size (MB): 0.21\n",
              "Estimated Total Size (MB): 3.23\n",
              "======================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 32\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 25\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-1\n",
        "\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32, scale=(0.75, 1.0), ratio=(1.0, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandAugment(num_ops=1, magnitude=8),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxpkuckJ2VkX",
        "outputId": "af1dea31-cf8f-4a5c-ec75-6976feea37ce"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "clip_norm = True\n",
        "lr_schedule = lambda t: np.interp([t], [0, EPOCHS*2//5, EPOCHS*4//5, EPOCHS], \n",
        "                                  [0, 0.01, 0.01/20.0, 0])[0]\n",
        "\n",
        "model = nn.DataParallel(model, device_ids=[0]).cuda()\n",
        "opt = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss, train_acc, n = 0, 0, 0\n",
        "    for i, (X, y) in enumerate(trainloader):\n",
        "        model.train()\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "\n",
        "        lr = lr_schedule(epoch + (i + 1)/len(trainloader))\n",
        "        opt.param_groups[0].update(lr=lr)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip_norm:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        train_acc += (output.max(1)[1] == y).sum().item()\n",
        "        n += y.size(0)\n",
        "        \n",
        "    model.eval()\n",
        "    test_acc, m = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(testloader):\n",
        "            X, y = X.cuda(), y.cuda()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(X)\n",
        "            test_acc += (output.max(1)[1] == y).sum().item()\n",
        "            m += y.size(0)\n",
        "\n",
        "    print(f'ViT: Epoch: {epoch} | Train Acc: {train_acc/n:.4f}, Test Acc: {test_acc/m:.4f}, Time: {time.time() - start:.1f}, lr: {lr:.6f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8wJMut6Nixb",
        "outputId": "f2450c50-3734-42d9-e411-eac1b0d9b22f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViT: Epoch: 0 | Train Acc: 0.2028, Test Acc: 0.2707, Time: 60.5, lr: 0.001000\n",
            "ViT: Epoch: 1 | Train Acc: 0.2526, Test Acc: 0.2940, Time: 60.4, lr: 0.002000\n",
            "ViT: Epoch: 2 | Train Acc: 0.2756, Test Acc: 0.3222, Time: 67.5, lr: 0.003000\n",
            "ViT: Epoch: 3 | Train Acc: 0.2992, Test Acc: 0.3556, Time: 61.9, lr: 0.004000\n",
            "ViT: Epoch: 4 | Train Acc: 0.3301, Test Acc: 0.3938, Time: 61.9, lr: 0.005000\n",
            "ViT: Epoch: 5 | Train Acc: 0.3716, Test Acc: 0.4289, Time: 62.7, lr: 0.006000\n",
            "ViT: Epoch: 6 | Train Acc: 0.4067, Test Acc: 0.4792, Time: 62.4, lr: 0.007000\n",
            "ViT: Epoch: 7 | Train Acc: 0.4346, Test Acc: 0.4870, Time: 62.7, lr: 0.008000\n",
            "ViT: Epoch: 8 | Train Acc: 0.4550, Test Acc: 0.5132, Time: 62.6, lr: 0.009000\n",
            "ViT: Epoch: 9 | Train Acc: 0.4673, Test Acc: 0.5193, Time: 61.6, lr: 0.010000\n",
            "ViT: Epoch: 10 | Train Acc: 0.4735, Test Acc: 0.5441, Time: 62.8, lr: 0.009050\n",
            "ViT: Epoch: 11 | Train Acc: 0.4948, Test Acc: 0.5488, Time: 62.8, lr: 0.008100\n",
            "ViT: Epoch: 12 | Train Acc: 0.5088, Test Acc: 0.5655, Time: 62.4, lr: 0.007150\n",
            "ViT: Epoch: 13 | Train Acc: 0.5261, Test Acc: 0.5551, Time: 62.7, lr: 0.006200\n",
            "ViT: Epoch: 14 | Train Acc: 0.5340, Test Acc: 0.5854, Time: 61.5, lr: 0.005250\n",
            "ViT: Epoch: 15 | Train Acc: 0.5436, Test Acc: 0.5837, Time: 62.8, lr: 0.004300\n",
            "ViT: Epoch: 16 | Train Acc: 0.5521, Test Acc: 0.5939, Time: 62.8, lr: 0.003350\n",
            "ViT: Epoch: 17 | Train Acc: 0.5682, Test Acc: 0.6074, Time: 62.3, lr: 0.002400\n",
            "ViT: Epoch: 18 | Train Acc: 0.5782, Test Acc: 0.6182, Time: 62.8, lr: 0.001450\n",
            "ViT: Epoch: 19 | Train Acc: 0.5884, Test Acc: 0.6240, Time: 61.3, lr: 0.000500\n",
            "ViT: Epoch: 20 | Train Acc: 0.5994, Test Acc: 0.6304, Time: 63.2, lr: 0.000400\n",
            "ViT: Epoch: 21 | Train Acc: 0.5975, Test Acc: 0.6303, Time: 63.4, lr: 0.000300\n",
            "ViT: Epoch: 22 | Train Acc: 0.6013, Test Acc: 0.6319, Time: 63.3, lr: 0.000200\n",
            "ViT: Epoch: 23 | Train Acc: 0.6051, Test Acc: 0.6338, Time: 63.2, lr: 0.000100\n",
            "ViT: Epoch: 24 | Train Acc: 0.6041, Test Acc: 0.6362, Time: 63.8, lr: 0.000000\n"
          ]
        }
      ]
    }
  ]
}